{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", download = True ,transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", download = True ,transform = transforms.Compose([transforms.ToTensor()]), train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, \n",
    "                                       shuffle=True)\n",
    "\n",
    "#bigger datasets, bs matters much more\n",
    "\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, \n",
    "                                       shuffle=True)\n",
    "            #need to be general so NN doesn't overfit to a single num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 5, 7, 6, 5, 2, 9, 6, 8, 6])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)\n",
    "\n",
    "#pytorch has a 1 there! need to know your shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ab49b00>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a87dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view([28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#balancing data? \n",
    "\n",
    "#could change class weights, or just make more balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #initializes nn.Module, see text based version for clarification\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 64) #flattened img, 3 layers of 64 connected neurons\n",
    "        self.fc2 = nn.Linear(64, 64) #flattened img, 3 layers of 64 connected neurons\n",
    "        self.fc3 = nn.Linear(64, 64) #flattened img, 3 layers of 64 connected neurons\n",
    "        self.fc4 = nn.Linear(64, 10) #flattened img, 3 layers of 64 connected neurons\n",
    "        \n",
    "        \n",
    "    def forward(self, x): #how data will flow through network\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return torch.nn.functional.log_softmax(x, dim = 1)\n",
    "    \n",
    "\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttributeError: cannot assign module before Module.__init__() call\n",
    "\n",
    "means that you haven't called super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(1, 28*28) #need the 1 in front!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2590, -2.3479, -2.3202, -2.2663, -2.3625, -2.3430, -2.1739, -2.3599,\n",
       "         -2.3595, -2.2523]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is CosineAnnealingLR, no modifications\n",
    "\n",
    "class the_reference(optim.lr_scheduler.CosineAnnealingLR):\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla NN, SGD / CosineAnnealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0029, grad_fn=<NllLossBackward>)\n",
      "tensor(9.7998e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7212e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#vanilla\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "lr_ = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr_)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=lr_ )\n",
    "\n",
    "epochs = 12\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        #data is a batch of features, labels\n",
    "        X,y = data\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step() #alter learning rate\n",
    "        \n",
    "        net.zero_grad() #reset gradient at each iteration\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        loss = nn.functional.nll_loss(output, y) # output is scalar, using nll\n",
    "        #if onehot, you can use MSE \n",
    "        \n",
    "        loss.backward() #back proprogates gradients\n",
    "        #can just iterate over net.parameters \n",
    "        \n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the modified SGD Optimizer I came up with - note the tot_epochs, lr_decay_factor, and per_epochs arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "required = 0.001\n",
    "\n",
    "class mod_SGD(torch.optim.SGD):\n",
    "\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, per_epochs = 3, tot_epochs = 12, lr_decay_factor = .2):\n",
    "        #added decay factor, total epoch, and per epochs clauses\n",
    "        self.per_epochs = per_epochs\n",
    "        self.tot_epochs = tot_epochs \n",
    "        self.lr_decay_factor = lr_decay_factor\n",
    "        \n",
    "        \n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(torch.optim.SGD, self).__init__(params, defaults)\n",
    "      \n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(torch.optim.SGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        epoch_period = 0 #counter to use in the decay factor for warm restarts\n",
    "        \n",
    "        if self.tot_epochs % self.per_epochs == 0:\n",
    "            epoch_period += 1\n",
    "             \n",
    "    \n",
    "        \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            if self.tot_epochs % self.per_epochs == 0:\n",
    "                #added this line here to reset the initial learning rate\n",
    "                    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['initial_lr'] * math.exp(-(lr_decay_factor*self.epoch_period))\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "                    \n",
    "\n",
    "                p.data.add_(-group['lr'], d_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#vanilla\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "lr_ = 0.01\n",
    "\n",
    "optimizer = mod_SGD(net.parameters(), lr = lr_)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=lr_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN With Modified SGD, Standard Cosine Annealing\n",
    "\n",
    "\n",
    "notes: Looking through, I haven't implemented a 'minimum learning rate' parameter as specified in the paper - perhaps this is why the error bounces around so much? I would have though the model would get closer and closer, not have a huge jump in loss (from .0050 to .2554), as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(8.6278e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0047, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9073e-07, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6591e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0038, grad_fn=<NllLossBackward>)\n",
      "tensor(6.9868e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1896e-05, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "lr_ = 0.01\n",
    "\n",
    "\n",
    "for epoch in range(optimizer.tot_epochs):\n",
    "    for data in trainset:\n",
    "        #data is a batch of features, labels\n",
    "        X,y = data\n",
    "        \n",
    "        \n",
    "        #if epoch is equal to per_epoch, then set the lr_ back to the default(?) lr\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step() #alter learning rate\n",
    "        \n",
    "        net.zero_grad() #reset gradient at each iteration\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        loss = nn.functional.nll_loss(output, y) # output is scalar, using nll\n",
    "        #if onehot, you can use MSE \n",
    "        \n",
    "        loss.backward() #back proprogates gradients\n",
    "        #can just iterate over net.parameters \n",
    "        \n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your modifed SGD and COSINE W WARM RESTARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = mod_SGD(net.parameters(), lr = lr_)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0418e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2540e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(12):\n",
    "    for data in trainset:\n",
    "        #data is a batch of features, labels\n",
    "        X,y = data\n",
    "        \n",
    "        \n",
    "        #if epoch is equal to per_epoch, then set the lr_ back to the default(?) lr\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step() #alter learning rate\n",
    "        \n",
    "        net.zero_grad() #reset gradient at each iteration\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        loss = nn.functional.nll_loss(output, y) # output is scalar, using nll\n",
    "        #if onehot, you can use MSE \n",
    "        \n",
    "        loss.backward() #back proprogates gradients\n",
    "        #can just iterate over net.parameters \n",
    "        \n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr_)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTORCH SGDR BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9359e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2383e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4543e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2888e-06, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2452e-07, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1730e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(12):\n",
    "    for data in trainset:\n",
    "        #data is a batch of features, labels\n",
    "        X,y = data\n",
    "        \n",
    "        #if epoch is equal to per_epoch, then set the lr_ back to the default(?) lr\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        net.zero_grad() #reset gradient at each iteration\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        loss = nn.functional.nll_loss(output, y) # output is scalar, using nll\n",
    "        #if onehot, you can use MSE \n",
    "        \n",
    "        loss.backward() #back proprogates gradients\n",
    "        #can just iterate over net.parameters \n",
    "        \n",
    "        \n",
    "    print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the CosineWithWarmRestarts is superior to my hodgepodge implementation. You can see that it immediately hits a low loss. We can see the loss come back up slightly, but that's more due to the small size of the dataset - model already hitting an the limitations of the data!\n",
    "\n",
    "I'm curious to see how the Pytorch team managed to implement it into the lr_scheduler, as I found it easier to implement it into SGD (probably not a good decision on my part). \n",
    "\n",
    "Below this is my attempt at modifiying CosineAnnealingWarmRes, but decided to keep my modification to SGD as shown above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mod_CosineAnnealingWarmRes(torch.optim.lr_scheduler.CosineAnnealingLR):\n",
    "\n",
    "    def __init__(self, optimizer, T_max, T_O, T_i, eta_min=0, last_epoch=-1, btwn_restarts = 3):\n",
    "            self.T_max = T_max\n",
    "            self.eta_min = eta_min\n",
    "            self.T_O = T_O\n",
    "            self.T_i = T_i\n",
    "            self.last_epoch = last_epoch\n",
    "            self.btwn_restarts = btwn_restarts\n",
    "            super(torch.optim.lr_scheduler.CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "            \n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    \n",
    "    def step(self, epoch = None):\n",
    "        \n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            t_curr = 1 \n",
    "        else:\n",
    "            if epoch > self.T_O:\n",
    "                self.T_i = epoch % self.T_O\n",
    "            else:\n",
    "                self.T_i = self.T_O\n",
    "                t_curr = epoch \n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_ = 0.01\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr_)\n",
    "\n",
    "scheduler = mod_CosineAnnealingWarmRes(optimizer, T_max = 3, T_i=3, T_O=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(12):\n",
    "    for data in trainset:\n",
    "        #data is a batch of features, labels\n",
    "        X,y = data\n",
    "        \n",
    "        \n",
    "        #if epoch is equal to per_epoch, then set the lr_ back to the default(?) lr\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step() #alter learning rate\n",
    "        \n",
    "        net.zero_grad() #reset gradient at each iteration\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        loss = nn.functional.nll_loss(output, y) # output is scalar, using nll\n",
    "        #if onehot, you can use MSE \n",
    "        \n",
    "        loss.backward() #back proprogates gradients\n",
    "        #can just iterate over net.parameters \n",
    "        \n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
